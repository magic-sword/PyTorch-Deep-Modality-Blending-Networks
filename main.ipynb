{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27938935",
   "metadata": {},
   "source": [
    "# 概要\n",
    "\n",
    "浅田 稔らによって発表されたDeep Modality Blending Networks(DMBN)は、複数の感覚器からの入力を1つに統合して学習することで、模倣行動のような能力を獲得することが示された。\n",
    "* [Imitation and mirror systems in robots through Deep Modality Blending Networks](https://www.sciencedirect.com/science/article/pii/S0893608021004329?via%3Dihub)\n",
    "* [Deep-Modality-Blending-Networks](https://github.com/myunusseker/Deep-Modality-Blending-Networks)\n",
    "\n",
    "このモデルは非常に興味深く、AIが行動の概念を学習できる可能性がある\n",
    "\n",
    "ここでは、モデルの実装をオブジェクト指向で再利用しやすいpytorchで行い、簡単に利用できるように\\[pytorch_dmbn\\]のpipライブラリとしてまとめる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713eac7",
   "metadata": {},
   "source": [
    "# 1.ライブラリの準備\n",
    "\n",
    "ローカルの実装修正をすぐに反映するため、pytorch_dmbnのライブラリとして再インストールする\n",
    "\n",
    "* --upgradeと--force-reinstallは再インストールを保証\n",
    "* --no-depsは依存関係の再インストールを回避します。<br>\n",
    "そうでなければ、pipがNumpyや他の大きなパッケージを再コンパイルし始めるという問題に遭遇するかもしれません。\n",
    "* -e コマンドは開発モードでパッケージをインストールするので、ソースコードを変更するとすぐ反映されて便利です。\n",
    "* . 現在のフォルダからインストールする\n",
    "\n",
    "参考\n",
    "* [[1]](https://www.web-dev-qa-db-ja.com/ja/python/%E7%8F%BE%E5%9C%A8%E3%81%AE%E3%83%90%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3%E3%82%92%E5%BC%B7%E5%88%B6%E7%9A%84%E3%81%AB%E5%86%8D%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AF%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%81%8B%EF%BC%9F/1043267345/) 現在のバージョンを強制的に再インストールすることはできますか？\n",
    "* [[2]](https://qiita.com/propella/items/803923b2ff02482242cd) Python でパッケージを開発して配布する標準的な方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce66dc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///workspace\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: pytorch-dmbn\n",
      "  Attempting uninstall: pytorch-dmbn\n",
      "    Found existing installation: pytorch-dmbn 0.0.1\n",
      "    Uninstalling pytorch-dmbn-0.0.1:\n",
      "      Successfully uninstalled pytorch-dmbn-0.0.1\n",
      "  Running setup.py develop for pytorch-dmbn\n",
      "Successfully installed pytorch-dmbn\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-deps --force-reinstall -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a350763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TestPyPIからインストールする\n",
    "# !pip install pytorch_dmbn --index-url https://test.pypi.org/simple/ pytorch_dmbn\n",
    "# import pytorch_dmbn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471dae5",
   "metadata": {},
   "source": [
    "モデルの利用に必要なインポートを記述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc51c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_dmbn.nn import BlendingEncoder\n",
    "from pytorch_dmbn.nn import TimeDistributed\n",
    "from pytorch_dmbn.nn import TimeAverageEncoder\n",
    "from pytorch_dmbn.nn import ImageEncoder\n",
    "from pytorch_dmbn.nn import ImageDecoder\n",
    "from pytorch_dmbn.nn import LinearEncoder\n",
    "from pytorch_dmbn.nn import CNPsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59a1beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2.1.0a0+fe05266\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# GPUデバイス読み込み\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c3f59",
   "metadata": {},
   "source": [
    "* [tensorflowのEmbedding レイヤーは何をするか？](https://qiita.com/9ryuuuuu/items/e4ee171079ffa4b87424)\n",
    "    * GlobalAveragePooling1D() レイヤーは、Embedding レイヤーで得られた値を平均値で圧縮する。\n",
    "    * 出力は比べてコンパクトになるが、情報圧縮により単語の前後関係が失われることは気に留めるべきである。\n",
    "* [PyTorchでGlobal Max PoolingとGlobal Average Poolingを使う方法](https://www.xn--ebkc7kqd.com/entry/pytorch-pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4c58e",
   "metadata": {},
   "source": [
    "## メモリ使用状況の確認\n",
    "\n",
    "* [【Python基礎】psutilによるメモリの使用状況の取得とJupyter Notebookにメモリ使用状況を表示する方法](https://3pysci.com/memorysize-1/)\n",
    "* [jupyter notebook上でPCのGPUチェック](https://qiita.com/Ringa_hyj/items/956e89e46aec3fb855b0)\n",
    "* [nvidia-smiコマンドの詳細ついて](https://qiita.com/miyamotok0105/items/1b34e548f96ef7d40370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b05d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 15 18:39:22 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A2000 12GB          On  | 00000000:01:00.0 Off |                  Off |\r\n",
      "| 31%   52C    P5              25W /  70W |     12MiB / 12282MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPU情報\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf718f2",
   "metadata": {},
   "source": [
    "# 2.モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# デフォルトのdtypeを設定する\n",
    "torch.set_default_dtype(torch.float)\n",
    "print(torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293af036",
   "metadata": {},
   "source": [
    "### DMBNを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57eeda3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): ImageDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (1): ReLU()\n",
       "      (2): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (4): ReLU()\n",
       "      (5): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (7): ReLU()\n",
       "      (8): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (10): ReLU()\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (13): ReLU()\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (16): ReLU()\n",
       "      (17): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (18): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (19): ReLU()\n",
       "      (20): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (21): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=216, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=216, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=32, out_features=14, bias=True)\n",
       "      (9): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# エンコーダを並べる\n",
    "encoders = [\n",
    "    ImageEncoder(image_size = (128,128), channels = [4,32,64,64,128,128,256], linear_features = 128), # 画像データのエンコーダ\n",
    "    LinearEncoder(channels = [8,32,64,64,128,128,256,128]) # 関節データのエンコーダ\n",
    "]\n",
    "encoders = [TimeAverageEncoder(encoder) for encoder in encoders]\n",
    "\n",
    "# エンコーダの出力を、1つの層にまとめるBlendingEncoderへ入力する\n",
    "blending_encoder = BlendingEncoder(encoders, in_features=129, linear_features=1024,)\n",
    "blending_encoder.to(device) # GPUにロード\n",
    "\n",
    "# デコーダーの出力は、入力に対する平均と分散の推測値\n",
    "# 従って、デコーダーの出力チャンネル数は、入力チャンネル数の倍になる(平均と分散をそれぞれ出力するため)\n",
    "decoders = nn.ModuleList([\n",
    "    ImageDecoder(in_features=1024, channels=[256,128,128,64,64,32,16,8,6], conv_kernel_size=3, upsample_scale=2, image_size=(128,128)), # 画像データのデコーダ\n",
    "    LinearEncoder(channels = [1024,512,216,128,32,14]) # 関節データのデコーダ\n",
    "])\n",
    "decoders.to(device) # GPUにロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18fd8a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# モデルがGPUにあることを確認\n",
    "# https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180\n",
    "print(next(blending_encoder.parameters()).is_cuda)\n",
    "print(next(decoders.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ef55f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load latest number: 00000010\n",
      "10\n",
      "load latest number: 00000010\n"
     ]
    }
   ],
   "source": [
    "# 保存済みのモデルがあれば読み込み\n",
    "from pytorch_dmbn.utils.save import ModelSaver\n",
    "saver = ModelSaver()\n",
    "\n",
    "if(saver.is_exist_file('blending_encoder')):\n",
    "    blending_encoder, blending_encoder_num = saver.load_latest('blending_encoder')\n",
    "    print(blending_encoder_num)\n",
    "if(saver.is_exist_file('decoders')):\n",
    "    decoders, decoders_num = saver.load_latest('decoders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea59d4",
   "metadata": {},
   "source": [
    "# 3. トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cff305a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの準備\n",
    "# 実装したDMBNDatasetは、参考元のDeep-Modality-Blending-Networksのgitからデータセットをダウンロードする\n",
    "from pytorch_dmbn.utils.data import DMBNDataset\n",
    "dataset = DMBNDataset()\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f4b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "cnps_loss = CNPsLoss()\n",
    "\n",
    "# Adamオプティマイザの定義\n",
    "optimizer = optim.Adam(\n",
    "    list(blending_encoder.parameters()) + list(decoders.parameters()),\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e82642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングステップを定義\n",
    "def train_one_step(train_dataloader):\n",
    "    for i , (\n",
    "            observation,\n",
    "            observation_pose,\n",
    "            target_X,\n",
    "            img_coef,\n",
    "            pose_coef,\n",
    "            target_Y,\n",
    "            target_Y_pose,\n",
    "            data_id,\n",
    "            target_time\n",
    "        ) in enumerate(train_dataloader):\n",
    "        # テンソルのデバイスを設定する\n",
    "        observation = observation.to(device)\n",
    "        observation_pose = observation_pose.to(device)\n",
    "        target_X = target_X.to(device)\n",
    "        img_coef = img_coef.to(device)\n",
    "        pose_coef = pose_coef.to(device)\n",
    "        target_Y = target_Y.to(device)\n",
    "        target_Y_pose = target_Y_pose.to(device)\n",
    "        target_time = target_time.to(device)\n",
    "        \n",
    "        imputs = [observation, observation_pose]\n",
    "        coefficients = [img_coef, pose_coef]\n",
    "        out = blending_encoder(imputs, coefficients, target_time)\n",
    "\n",
    "        img = decoders[0](out)\n",
    "        pose = decoders[1](out)\n",
    "\n",
    "        # 損失の計算\n",
    "        loss = cnps_loss(img, target_Y) + cnps_loss(pose, target_Y_pose)\n",
    "\n",
    "        # 勾配をゼロに初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # バックプロパゲーション：勾配を計算\n",
    "        loss.backward()\n",
    "\n",
    "        # オプティマイザステップ：パラメータを更新\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d98dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングを実行\n",
    "max_training_step = 11\n",
    "save_step = 10\n",
    "for step in range(max_training_step):\n",
    "    train_one_step(train_dataloader)\n",
    "    \n",
    "    # データローダーの観測数を再設定\n",
    "    dataset.rand_obs_num()\n",
    "    \n",
    "    # 区切り毎にデータを保存\n",
    "    if(step % save_step == 0):\n",
    "        saver.save(blending_encoder, 'blending_encoder', step)\n",
    "        saver.save(decoders, 'decoders', step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
